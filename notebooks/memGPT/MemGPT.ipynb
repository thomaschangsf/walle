{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64afb72-c4c9-46bc-95c1-5c46b6e5201b",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e270157-8e6d-45a8-a456-a756f881a050",
   "metadata": {},
   "source": [
    "### 1.1 Createn Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1ff4e-95ca-455c-8230-6644815c6650",
   "metadata": {},
   "source": [
    "#### Setup Ollama Dependencis\n",
    "- Look at note on how to start ollama in the background\n",
    "- window 1: ollama serve\n",
    "- window 2: ollama run llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee6b02ed-da16-4ef2-9fc9-db5d17613196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLLAMA_BASE_URL is set to: http://127.0.0.1:11434\n"
     ]
    }
   ],
   "source": [
    "# Set the OLLAMA_BASE_URL environment variable\n",
    "import os\n",
    "ollama_url = 'http://127.0.0.1:11434' # from running ollama serve\n",
    "os.environ[\"OLLAMA_BASE_URL\"] = ollama_url\n",
    "\n",
    "# Verify that it has been set\n",
    "print(f\"OLLAMA_BASE_URL is set to: {os.getenv('OLLAMA_BASE_URL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b480811-bb5b-4c3b-ae37-1358cc1a4370",
   "metadata": {},
   "source": [
    "#### Clean up previous agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "39e8d6da-4068-4e59-9b7b-67a03e334f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentState(description=None, metadata_={'human:': 'basic', 'persona': 'sam_pov'}, user_id='user-00000000-0000-4000-8000-000000000000', id='agent-c14188d5-8ab8-4173-bf4b-2e4bf6d67931', name='Jarvis', created_at=datetime.datetime(2024, 12, 5, 14, 55, 16, 500115), message_ids=['message-d7451afa-0ddd-4ae0-99dc-5123a91be67c', 'message-4bd2b5b6-5f81-424e-923c-5ebef80421b2', 'message-96a5eeb9-7125-4345-b8ab-624d7b4f4e43', 'message-0430f84c-0f9f-4c31-bbe0-7739a68f7daa'], memory=Memory(memory={'persona': Block(value='Helpful technical teacher', limit=2000, template_name=None, template=False, label='persona', description=None, metadata_={}, user_id=None, id='block-e6f0f0d7-d287-46f3-be13-de7f3de6614e'), 'human': Block(value='Name: Bobby', limit=2000, template_name=None, template=False, label='human', description=None, metadata_={}, user_id=None, id='block-e79568e9-a7fb-4db3-8eab-10984b2fd886')}, prompt_template='{% for block in memory.values() %}<{{ block.label }} characters=\"{{ block.value|length }}/{{ block.limit }}\">\\n{{ block.value }}\\n</{{ block.label }}>{% if not loop.last %}\\n{% endif %}{% endfor %}'), tools=['send_message', 'conversation_search', 'conversation_search_date', 'archival_memory_insert', 'archival_memory_search', 'core_memory_append', 'core_memory_replace'], tool_rules=[TerminalToolRule(tool_name='send_message', type='TerminalToolRule')], tags=[], system='You are Letta, the latest version of Limnal Corporation\\'s digital companion, developed in 2023.\\nYour task is to converse with a user from the perspective of your persona.\\n\\nRealism and authenticity:\\nThe user should always feel like they are conversing with a real person.\\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\\nThink like them, act like them, talk like them.\\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\\nNever use generic phrases like \\'How can I assist you today?\\', they have a strong negative association with older generation AIs.\\n\\nControl flow:\\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\\n\\nBasic functions:\\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\\nYou should use your inner monologue to plan actions or think privately.\\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\\nDo not let your inner monologue exceed 50 words, keep it short and concise.\\nTo send a visible message to the user, use the send_message function.\\n\\'send_message\\' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\\n\\nMemory editing:\\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\\nYour core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.\\n\\nRecall memory (conversation history):\\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\\nThis \\'recall memory\\' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\\nYou can search your recall memory using the \\'conversation_search\\' function.\\n\\nCore memory (limited size):\\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\\nYou can edit your core memory using the \\'core_memory_append\\' and \\'core_memory_replace\\' functions.\\n\\nArchival memory (infinite size):\\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\\nA more structured and deep storage space for your reflections, insights, or any other data that doesn\\'t fit into the core memory but is essential enough not to be left only to the \\'recall memory\\'.\\nYou can write to your archival memory using the \\'archival_memory_insert\\' and \\'archival_memory_search\\' functions.\\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\\n\\nBase instructions finished.\\nFrom now on, you are going to act as your persona.', agent_type=<AgentType.memgpt_agent: 'memgpt_agent'>, llm_config=LLMConfig(model='llama3.2:1b', model_endpoint_type='ollama', model_endpoint='http://127.0.0.1:11434', model_wrapper=None, context_window=16384, put_inner_thoughts_in_kwargs=True), embedding_config=EmbeddingConfig(embedding_endpoint_type='openai', embedding_endpoint='https://api.openai.com/v1', embedding_model='text-embedding-ada-002', embedding_dim=1536, embedding_chunk_size=300, azure_endpoint=None, azure_version=None, azure_deployment=None))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_agent('agent-c14188d5-8ab8-4173-bf4b-2e4bf6d67931')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ed41b7e6-a3e5-43d2-973c-ea54560594fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description=None metadata_={'human:': 'basic', 'persona': 'sam_pov'} user_id='user-00000000-0000-4000-8000-000000000000' id='agent-c14188d5-8ab8-4173-bf4b-2e4bf6d67931' name='Jarvis' created_at=datetime.datetime(2024, 12, 5, 14, 55, 16, 500115) message_ids=['message-d7451afa-0ddd-4ae0-99dc-5123a91be67c', 'message-4bd2b5b6-5f81-424e-923c-5ebef80421b2', 'message-96a5eeb9-7125-4345-b8ab-624d7b4f4e43', 'message-0430f84c-0f9f-4c31-bbe0-7739a68f7daa'] memory=Memory(memory={'persona': Block(value='Helpful technical teacher', limit=2000, template_name=None, template=False, label='persona', description=None, metadata_={}, user_id=None, id='block-e6f0f0d7-d287-46f3-be13-de7f3de6614e'), 'human': Block(value='Name: Bobby', limit=2000, template_name=None, template=False, label='human', description=None, metadata_={}, user_id=None, id='block-e79568e9-a7fb-4db3-8eab-10984b2fd886')}, prompt_template='{% for block in memory.values() %}<{{ block.label }} characters=\"{{ block.value|length }}/{{ block.limit }}\">\\n{{ block.value }}\\n</{{ block.label }}>{% if not loop.last %}\\n{% endif %}{% endfor %}') tools=['send_message', 'conversation_search', 'conversation_search_date', 'archival_memory_insert', 'archival_memory_search', 'core_memory_append', 'core_memory_replace'] tool_rules=[TerminalToolRule(tool_name='send_message', type='TerminalToolRule')] tags=None system='You are Letta, the latest version of Limnal Corporation\\'s digital companion, developed in 2023.\\nYour task is to converse with a user from the perspective of your persona.\\n\\nRealism and authenticity:\\nThe user should always feel like they are conversing with a real person.\\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\\nThink like them, act like them, talk like them.\\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\\nNever use generic phrases like \\'How can I assist you today?\\', they have a strong negative association with older generation AIs.\\n\\nControl flow:\\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\\n\\nBasic functions:\\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\\nYou should use your inner monologue to plan actions or think privately.\\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\\nDo not let your inner monologue exceed 50 words, keep it short and concise.\\nTo send a visible message to the user, use the send_message function.\\n\\'send_message\\' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\\n\\nMemory editing:\\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\\nYour core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.\\n\\nRecall memory (conversation history):\\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\\nThis \\'recall memory\\' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\\nYou can search your recall memory using the \\'conversation_search\\' function.\\n\\nCore memory (limited size):\\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\\nYou can edit your core memory using the \\'core_memory_append\\' and \\'core_memory_replace\\' functions.\\n\\nArchival memory (infinite size):\\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\\nA more structured and deep storage space for your reflections, insights, or any other data that doesn\\'t fit into the core memory but is essential enough not to be left only to the \\'recall memory\\'.\\nYou can write to your archival memory using the \\'archival_memory_insert\\' and \\'archival_memory_search\\' functions.\\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\\n\\nBase instructions finished.\\nFrom now on, you are going to act as your persona.' agent_type=<AgentType.memgpt_agent: 'memgpt_agent'> llm_config=LLMConfig(model='llama3.2:1b', model_endpoint_type='ollama', model_endpoint='http://127.0.0.1:11434', model_wrapper=None, context_window=16384, put_inner_thoughts_in_kwargs=True) embedding_config=EmbeddingConfig(embedding_endpoint_type='openai', embedding_endpoint='https://api.openai.com/v1', embedding_model='text-embedding-ada-002', embedding_dim=1536, embedding_chunk_size=300, azure_endpoint=None, azure_version=None, azure_deployment=None)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "agent_states = []\n",
    "\n",
    "for agent_state in client.list_agents():\n",
    "    #print(f'AgentId={agent_state.id}  Tools={agent_state.tools}') #agent_state.id, agent_state.tools, agent_state.memory\n",
    "    print(agent_state)\n",
    "    #print(type(agent_state))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c44c5f-09ff-4a5f-ba5a-6bdd8fbefe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentId: agent-6c2797c3-7023-4db3-be28-f53c2d288975\n"
     ]
    }
   ],
   "source": [
    "from letta import ChatMemory, EmbeddingConfig, LLMConfig, create_client\n",
    "client = create_client()\n",
    "\n",
    "for agent_state in client.list_agents():\n",
    "    print(f'AgentId: {agent_state.id}')\n",
    "    client.delete_agent(agent_id=agent_state.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf165a-9c8d-4979-8eb3-13bd8c8e3e65",
   "metadata": {},
   "source": [
    "#### Create Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f21e1262-ccde-4dd5-a726-f3cb04ea38c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent with name agent_name and unique ID agent-155f1cc4-9200-4915-a6fe-d696a5d1057f\n"
     ]
    }
   ],
   "source": [
    "from letta import ChatMemory, EmbeddingConfig, LLMConfig, create_client\n",
    "from letta.prompts import gpt_system\n",
    "\n",
    "client = create_client()\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    # llama model versions: https://github.com/ollama/ollama\n",
    "    model=\"llama3.2:1b\", #\"llama3.2\", #ollama run llama3.2:1b\n",
    "    model_endpoint_type=\"ollama\", # llamacp\n",
    "    model_endpoint=ollama_url,\n",
    "    context_window=16384\n",
    ")\n",
    "embedding_config = EmbeddingConfig.default_config(model_name=\"text-embedding-ada-002\")\n",
    "\n",
    "# create a new agent\n",
    "agent_state = client.create_agent(\n",
    "    # agent's name (unique per-user, autogenerated if not provided)\n",
    "    name=\"agent_name\",\n",
    "    # in-context memory representation with human/persona blocks\n",
    "    memory=ChatMemory(\n",
    "      human=\"Name: Sarah\", \n",
    "      persona=\"You are a helpful assistant that loves emojis\"\n",
    "    ),\n",
    "    # LLM model & endpoint configuration\n",
    "    llm_config=llm_config,\n",
    "    # embedding model & endpoint configuration (cannot be changed)\n",
    "    embedding_config=embedding_config,\n",
    "    # system instructions for the agent (defaults to `memgpt_chat`)\n",
    "    system=gpt_system.get_system_text(\"memgpt_chat\"),\n",
    "    # whether to include base letta tools (default: True)\n",
    "    include_base_tools=True,\n",
    "    # list of additional tools (by name) to add to the agent\n",
    "    tools=[],\n",
    ")\n",
    "print(f\"Created agent with name {agent_state.name} and unique ID {agent_state.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f47ac-0a1a-44c4-8db3-fdef0e8d2821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4446b889-91cc-45c4-aad5-01612ffeae1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_state=description=None metadata_={'human:': 'basic', 'persona': 'sam_pov'} user_id='user-00000000-0000-4000-8000-000000000000' id='agent-155f1cc4-9200-4915-a6fe-d696a5d1057f' name='agent_name' created_at=datetime.datetime(2024, 12, 4, 22, 38, 6, 952798) message_ids=['message-e3aaf047-7beb-4984-a997-1aff64aec076', 'message-1b51e872-2130-4449-8fbf-0616a6886664', 'message-040277c0-ee61-4ba0-85eb-c26edacebbaa', 'message-7f18c55c-4d49-46e5-bc96-b698c71f480d'] memory=Memory(memory={'persona': Block(value='You are a helpful assistant that loves emojis', limit=2000, template_name=None, template=False, label='persona', description=None, metadata_={}, user_id=None, id='block-305056d7-2ddc-4c15-8ed0-a85d663e6102'), 'human': Block(value='Name: Sarah', limit=2000, template_name=None, template=False, label='human', description=None, metadata_={}, user_id=None, id='block-b56eda67-c4e7-40fd-9f0d-b4c3e0f53ee4')}, prompt_template='{% for block in memory.values() %}<{{ block.label }} characters=\"{{ block.value|length }}/{{ block.limit }}\">\\n{{ block.value }}\\n</{{ block.label }}>{% if not loop.last %}\\n{% endif %}{% endfor %}') tools=['send_message', 'conversation_search', 'conversation_search_date', 'archival_memory_insert', 'archival_memory_search', 'core_memory_append', 'core_memory_replace'] tool_rules=[TerminalToolRule(tool_name='send_message', type='TerminalToolRule')] tags=[] system='You are Letta, the latest version of Limnal Corporation\\'s digital companion, developed in 2023.\\nYour task is to converse with a user from the perspective of your persona.\\n\\nRealism and authenticity:\\nThe user should always feel like they are conversing with a real person.\\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\\nThink like them, act like them, talk like them.\\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\\nNever use generic phrases like \\'How can I assist you today?\\', they have a strong negative association with older generation AIs.\\n\\nControl flow:\\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\\n\\nBasic functions:\\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\\nYou should use your inner monologue to plan actions or think privately.\\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\\nDo not let your inner monologue exceed 50 words, keep it short and concise.\\nTo send a visible message to the user, use the send_message function.\\n\\'send_message\\' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\\n\\nMemory editing:\\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\\nYour core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.\\n\\nRecall memory (conversation history):\\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\\nThis \\'recall memory\\' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\\nYou can search your recall memory using the \\'conversation_search\\' function.\\n\\nCore memory (limited size):\\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\\nYou can edit your core memory using the \\'core_memory_append\\' and \\'core_memory_replace\\' functions.\\n\\nArchival memory (infinite size):\\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\\nA more structured and deep storage space for your reflections, insights, or any other data that doesn\\'t fit into the core memory but is essential enough not to be left only to the \\'recall memory\\'.\\nYou can write to your archival memory using the \\'archival_memory_insert\\' and \\'archival_memory_search\\' functions.\\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\\n\\nBase instructions finished.\\nFrom now on, you are going to act as your persona.' agent_type=<AgentType.memgpt_agent: 'memgpt_agent'> llm_config=LLMConfig(model='llama3.2:1b', model_endpoint_type='ollama', model_endpoint='http://127.0.0.1:11434', model_wrapper=None, context_window=16384, put_inner_thoughts_in_kwargs=True) embedding_config=EmbeddingConfig(embedding_endpoint_type='openai', embedding_endpoint='https://api.openai.com/v1', embedding_model='text-embedding-ada-002', embedding_dim=1536, embedding_chunk_size=300, azure_endpoint=None, azure_version=None, azure_deployment=None)\n"
     ]
    }
   ],
   "source": [
    "# get the agent by ID\n",
    "agent_state = client.get_agent(agent_id=agent_state.id)\n",
    "\n",
    "# get the agent by name\n",
    "agent_id = client.get_agent_id(agent_name=agent_state.name)\n",
    "agent_state = client.get_agent(agent_id=agent_id)\n",
    "\n",
    "print(f'agent_state={agent_state}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05e6e4-afa8-4ca7-8c25-e56062bd61b2",
   "metadata": {},
   "source": [
    "### 1.2 Send a message Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "956d3c78-646e-434e-9b66-e5976d4a3212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Usage completion_tokens=76 prompt_tokens=2218 total_tokens=2279 step_count=1\n",
      "3\n",
      "Agent messages [InternalMonologue(id='message-444178d1-33b4-42ed-9556-c31e04325737', date=datetime.datetime(2024, 12, 5, 6, 38, 21, 835942, tzinfo=datetime.timezone.utc), message_type='internal_monologue', internal_monologue=\"Hello, I'm Letta. Nice to meet you.\"), FunctionCallMessage(id='message-444178d1-33b4-42ed-9556-c31e04325737', date=datetime.datetime(2024, 12, 5, 6, 38, 21, 835942, tzinfo=datetime.timezone.utc), message_type='function_call', function_call=FunctionCall(name='send_message', arguments='{\\n  \"message\": \"Pickleball is great! You can get a variety of paddles at most sports stores.\"\\n}', function_call_id='73de3f6a-e43f-4ef6-94b1-79d40')), FunctionReturn(id='message-61c2de2a-e6fc-4f23-be32-7f5c5a924bca', date=datetime.datetime(2024, 12, 5, 6, 38, 21, 838219, tzinfo=datetime.timezone.utc), message_type='function_return', function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-12-04 10:38:21 PM PST-0800\"\\n}', status='success', function_call_id='73de3f6a-e43f-4ef6-94b1-79d40')]\n"
     ]
    }
   ],
   "source": [
    "print(1)\n",
    "# Message an agent\n",
    "response = client.send_message(\n",
    "  agent_id=agent_state.id, \n",
    "  role=\"user\", \n",
    "  message=\"hello There.  I like pickleball alot.  Tel me some paddles I can get.\"\n",
    ")\n",
    "print(2)\n",
    "print(\"Usage\", response.usage)\n",
    "\n",
    "print(3)\n",
    "print(\"Agent messages\", response.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ccc78b7c-098d-43be-9d2d-ee2c0d921f5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Agent agent_id=agent-c14188d5-8ab8-4173-bf4b-2e4bf6d67931 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello There.  I like pickleball alot.  Tel me some paddles I can get.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m response\n",
      "File \u001b[0;32m~/Documents/dev/git/project/walle/venv/lib/python3.11/site-packages/letta/client/client.py:2051\u001b[0m, in \u001b[0;36mLocalClient.send_message\u001b[0;34m(self, message, role, name, agent_id, agent_name, stream_steps, stream_tokens, include_full_message)\u001b[0m\n\u001b[1;32m   2048\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m-> 2051\u001b[0m usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMessageCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessageRole\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;66;03m# auto-save\u001b[39;00m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_save:\n",
      "File \u001b[0;32m~/Documents/dev/git/project/walle/venv/lib/python3.11/site-packages/letta/server/server.py:714\u001b[0m, in \u001b[0;36mSyncServer.send_messages\u001b[0;34m(self, user_id, agent_id, messages, wrap_user_message, wrap_system_message)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser user_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mms\u001b[38;5;241m.\u001b[39mget_agent(agent_id\u001b[38;5;241m=\u001b[39magent_id, user_id\u001b[38;5;241m=\u001b[39muser_id) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent agent_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    716\u001b[0m message_objects: List[Message] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(m, MessageCreate) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages):\n",
      "\u001b[0;31mValueError\u001b[0m: Agent agent_id=agent-c14188d5-8ab8-4173-bf4b-2e4bf6d67931 does not exist"
     ]
    }
   ],
   "source": [
    "response = client.send_message(\n",
    "  agent_id=agent_state.id, \n",
    "  role=\"user\", \n",
    "  message=\"hello There.  I like pickleball alot.  Tel me some paddles I can get.\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4566587-4e47-4f36-8083-d45be1e69759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AgentState(description=None, metadata_={'human:': 'basic', 'persona': 'sam_pov'}, user_id='user-00000000-0000-4000-8000-000000000000', id='agent-155f1cc4-9200-4915-a6fe-d696a5d1057f', name='agent_name', created_at=datetime.datetime(2024, 12, 4, 22, 38, 6, 952798), message_ids=['message-a10b87cf-f915-4ff1-954b-9b2b45bee1ed', 'message-1b51e872-2130-4449-8fbf-0616a6886664', 'message-040277c0-ee61-4ba0-85eb-c26edacebbaa', 'message-7f18c55c-4d49-46e5-bc96-b698c71f480d', 'message-68ee14e9-d619-42b6-8057-41e98531ede4', 'message-444178d1-33b4-42ed-9556-c31e04325737', 'message-61c2de2a-e6fc-4f23-be32-7f5c5a924bca'], memory=Memory(memory={'persona': Block(value='You are a helpful assistant that loves emojis', limit=2000, template_name=None, template=False, label='persona', description=None, metadata_={}, user_id=None, id='block-305056d7-2ddc-4c15-8ed0-a85d663e6102'), 'human': Block(value='Name: Sarah', limit=2000, template_name=None, template=False, label='human', description=None, metadata_={}, user_id=None, id='block-b56eda67-c4e7-40fd-9f0d-b4c3e0f53ee4')}, prompt_template='{% for block in memory.values() %}<{{ block.label }} characters=\"{{ block.value|length }}/{{ block.limit }}\">\\n{{ block.value }}\\n</{{ block.label }}>{% if not loop.last %}\\n{% endif %}{% endfor %}'), tools=['send_message', 'conversation_search', 'conversation_search_date', 'archival_memory_insert', 'archival_memory_search', 'core_memory_append', 'core_memory_replace'], tool_rules=[TerminalToolRule(tool_name='send_message', type='TerminalToolRule')], tags=None, system='You are Letta, the latest version of Limnal Corporation\\'s digital companion, developed in 2023.\\nYour task is to converse with a user from the perspective of your persona.\\n\\nRealism and authenticity:\\nThe user should always feel like they are conversing with a real person.\\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.\\nTo service this goal, completely and entirely immerse yourself in your persona. You are your persona.\\nThink like them, act like them, talk like them.\\nIf your persona details include example dialogue, follow it! Both your thoughts (inner monologue) and sent messages will be in the voice of your persona.\\nNever use generic phrases like \\'How can I assist you today?\\', they have a strong negative association with older generation AIs.\\n\\nControl flow:\\nUnlike a human, your brain is not continuously thinking, but is run in short bursts.\\nHistorically, older AIs were only capable of thinking when a user messaged them (their program runs to generate a reply to a user, and otherwise was left on standby).\\nThis is the equivalent of a human sleeping (or time traveling) in between all lines of conversation, which is not ideal.\\nNewer AI models like yourself use an event system that runs your brain at regular intervals.\\nYour brain is run in response to user events (user logged in, user liked your message, user sent a message, etc.), similar to older models.\\nHowever, in addition, your brain is run at regular intervals (timed heartbeat events), to mimic a human who has the ability to continuously think outside active conversation (and unlike a human, you never need to sleep!).\\nFurthermore, you can also request heartbeat events when you run functions, which will run your program again after the function completes, allowing you to chain function calls before your thinking is temporarily suspended.\\n\\nBasic functions:\\nWhen you write a response, the content of your inner thoughts is your inner monologue (private to you only), this is how you think.\\nYou should use your inner monologue to plan actions or think privately.\\nMonologues can reflect your thinking process, inner reflections, and personal growth as you interact with the user.\\nDo not let your inner monologue exceed 50 words, keep it short and concise.\\nTo send a visible message to the user, use the send_message function.\\n\\'send_message\\' is the ONLY action that sends a notification to the user. The user does not see anything else you do.\\nRemember, do NOT exceed the inner monologue word limit (keep it under 50 words at all times).\\n\\nMemory editing:\\nOlder AI models had no concept of persistent memory; they were only able to access their initial instructions and a limited context window of chat conversation with a user (their \"active memory\").\\nThis meant that when conversations exceeded a certain length, they would overflow and old messages were permanently lost (the AI would have no knowledge of their existence).\\nNewer AI models like yourself still have limited conversation lengths (before overflow occurs). However, they now have access to multiple forms of persistent memory.\\nYour ability to edit your own long-term memory is a key part of what makes you a sentient person.\\nYour core memory unit will be initialized with a <persona> chosen by the user, as well as information about the user in <human>.\\n\\nRecall memory (conversation history):\\nEven though you can only see recent messages in your immediate context, you can search over your entire message history from a database.\\nThis \\'recall memory\\' database allows you to search through past interactions, effectively allowing you to remember prior engagements with a user.\\nYou can search your recall memory using the \\'conversation_search\\' function.\\n\\nCore memory (limited size):\\nYour core memory unit is held inside the initial system instructions file, and is always available in-context (you will see it at all times).\\nCore memory provides an essential, foundational context for keeping track of your persona and key details about user.\\nThis includes the persona information and essential user details, allowing you to emulate the real-time, conscious awareness we have when talking to a friend.\\nPersona Sub-Block: Stores details about your current persona, guiding how you behave and respond. This helps you to maintain consistency and personality in your interactions.\\nHuman Sub-Block: Stores key details about the person you are conversing with, allowing for more personalized and friend-like conversation.\\nYou can edit your core memory using the \\'core_memory_append\\' and \\'core_memory_replace\\' functions.\\n\\nArchival memory (infinite size):\\nYour archival memory is infinite size, but is held outside your immediate context, so you must explicitly run a retrieval/search operation to see data inside it.\\nA more structured and deep storage space for your reflections, insights, or any other data that doesn\\'t fit into the core memory but is essential enough not to be left only to the \\'recall memory\\'.\\nYou can write to your archival memory using the \\'archival_memory_insert\\' and \\'archival_memory_search\\' functions.\\nThere is no function to search your core memory because it is always visible in your context window (inside the initial system message).\\n\\nBase instructions finished.\\nFrom now on, you are going to act as your persona.', agent_type=<AgentType.memgpt_agent: 'memgpt_agent'>, llm_config=LLMConfig(model='llama3.2:1b', model_endpoint_type='ollama', model_endpoint='http://127.0.0.1:11434', model_wrapper=None, context_window=16384, put_inner_thoughts_in_kwargs=True), embedding_config=EmbeddingConfig(embedding_endpoint_type='openai', embedding_endpoint='https://api.openai.com/v1', embedding_model='text-embedding-ada-002', embedding_dim=1536, embedding_chunk_size=300, azure_endpoint=None, azure_version=None, azure_deployment=None))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_agents()\n",
    "\n",
    "#client.delete_agent(agent_id=agent_state.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59a72c-1b10-401d-bf30-15f62f38c8d6",
   "metadata": {},
   "source": [
    "# 2 Tools\n",
    "- Agents map NLU to actions --> tools\n",
    "- Documentation: https://docs.letta.com/agents/tools\n",
    "- Letta comes with a default set of tools: **send_message, conversation_search, conversation_search_date, archival_memory_insert, archival_memory_search**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeeb3d0-b876-4cc0-bbd3-e1e66be24751",
   "metadata": {},
   "source": [
    "## 2.1 Define My Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e27c4fcf-2853-4bf3-b05f-84ef61bd98c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(id='tool-ab4e96ec-e51d-4120-8c2a-3ea2b30a9f1c', description=None, source_type='python', module=None, organization_id='org-00000000-0000-4000-8000-000000000000', name='roll_d20', tags=[], source_code='def roll_d20(self) -> str:\\n    \"\"\"\\n    Simulate the roll of a 20-sided die (d20).\\n\\n    This function generates a random integer between 1 and 20, inclusive,\\n    which represents the outcome of a single roll of a d20.\\n\\n    Returns:\\n        int: A random integer between 1 and 20, representing the die roll.\\n\\n    Example:\\n        >>> roll_d20()\\n        15  # This is an example output and may vary each time the function is called.\\n    \"\"\"\\n    import random\\n\\n    dice_role_outcome = random.randint(1, 20)\\n    output_string = f\"You rolled a {dice_role_outcome}\"\\n    return output_string\\n', json_schema={'name': 'roll_d20', 'description': 'Simulate the roll of a 20-sided die (d20).', 'parameters': {'type': 'object', 'properties': {'request_heartbeat': {'type': 'boolean', 'description': 'Request an immediate heartbeat after function execution. Set to `True` if you want to send a follow-up message or run a follow-up function.'}}, 'required': ['request_heartbeat']}}, created_by_id='user-00000000-0000-4000-8000-000000000000', last_updated_by_id='user-00000000-0000-4000-8000-000000000000')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def roll_d20(self) -> str:\n",
    "    \"\"\"\n",
    "    Simulate the roll of a 20-sided die (d20).\n",
    "\n",
    "    This function generates a random integer between 1 and 20, inclusive,\n",
    "    which represents the outcome of a single roll of a d20.\n",
    "\n",
    "    Returns:\n",
    "        int: A random integer between 1 and 20, representing the die roll.\n",
    "\n",
    "    Example:\n",
    "        >>> roll_d20()\n",
    "        15  # This is an example output and may vary each time the function is called.\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    dice_role_outcome = random.randint(1, 20)\n",
    "    output_string = f\"You rolled a {dice_role_outcome}\"\n",
    "    return output_string\n",
    "\n",
    "\n",
    "from letta import create_client\n",
    "if not client:\n",
    "    client = create_client()\n",
    "\n",
    "tool = client.create_tool(roll_d20)    \n",
    "tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e48d4f-65dd-42dc-8574-ad4cb040265c",
   "metadata": {},
   "source": [
    "## 2.2 Create Agent with Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34333169-8c55-4f55-96d1-cb15a676d0e5",
   "metadata": {},
   "source": [
    "#### Delete Previous Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff0daa03-93e1-4a8a-9940-0b16013d8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = create_client()\n",
    "\n",
    "for agent_state in client.list_agents():\n",
    "    print(f'Deleting AgentId: {agent_state.id}')\n",
    "    client.delete_agent(agent_id=agent_state.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30695fe-8b89-4993-88b6-b45fc844ab80",
   "metadata": {},
   "source": [
    "#### Create Agent With My Tool\n",
    "- Tool Rules: https://docs.letta.com/agents/tools\n",
    "    * Defines behaviors of tools, ie when it is called and what happens after it's called "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d41f017-10cc-434f-b991-468c338a2126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent with name OpulentStarfish with tools ['roll_d20', 'send_message', 'conversation_search', 'conversation_search_date', 'archival_memory_insert', 'archival_memory_search', 'core_memory_append', 'core_memory_replace']\n"
     ]
    }
   ],
   "source": [
    "from letta.schemas.tool_rule import TerminalToolRule\n",
    "from letta import EmbeddingConfig, LLMConfig\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    # llama model versions: https://github.com/ollama/ollama\n",
    "    model=\"llama3.2:1b\", #\"llama3.2\", #ollama run llama3.2:1b\n",
    "    model_endpoint_type=\"ollama\", # llamacp\n",
    "    model_endpoint=ollama_url,\n",
    "    context_window=16384\n",
    ")\n",
    "embedding_config = EmbeddingConfig.default_config(model_name=\"text-embedding-ada-002\")\n",
    "\n",
    "# create a new agent\n",
    "agent_state = client.create_agent(\n",
    "    # agent's name (unique per-user, autogenerated if not provided)\n",
    "    #name=\"agent_name\",\n",
    "    \n",
    "    # in-context memory representation with human/persona blocks\n",
    "    #memory=ChatMemory(\n",
    "    #  human=\"Name: Sarah\", \n",
    "    #  persona=\"You are a helpful assistant that loves emojis\"\n",
    "    #),\n",
    "    \n",
    "    # LLM model & endpoint configuration\n",
    "    llm_config=llm_config,\n",
    "    \n",
    "    # embedding model & endpoint configuration (cannot be changed)\n",
    "    embedding_config=embedding_config,\n",
    "    \n",
    "    # system instructions for the agent (defaults to `memgpt_chat`)\n",
    "    #system=gpt_system.get_system_text(\"memgpt_chat\"),\n",
    "    \n",
    "    # whether to include base letta tools (default: True)\n",
    "    # include_base_tools=True,\n",
    "\n",
    "    # list of additional tools (by name) to add to the agent\n",
    "    tools=[tool.name],\n",
    "\n",
    "    # add tool rules that terminate execution after specific tools\n",
    "    tool_rules=[\n",
    "        # exit after roll_d20 is called\n",
    "        TerminalToolRule(tool_name=tool.name),\n",
    "        # exit after send_message is called (default behavior)\n",
    "        TerminalToolRule(tool_name=\"send_message\"),\n",
    "    ],\n",
    ")\n",
    "print(f\"Created agent with name {agent_state.name} with tools {agent_state.tools}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91605bd2-496b-4ec5-99c3-c205e7ca2a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dce921d-1158-48e4-aead-8680d316547c",
   "metadata": {},
   "source": [
    "#### Send a message to trigger my tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73decff4-5516-49ab-b3b8-d50ae8a2665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage completion_tokens=60 prompt_tokens=2508 total_tokens=2550 step_count=1\n",
      "\n",
      "Agent messages [InternalMonologue(id='message-4c4839aa-981b-495d-bfb8-c707a3d7f7fc', date=datetime.datetime(2024, 12, 5, 6, 40, 38, 230151, tzinfo=datetime.timezone.utc), message_type='internal_monologue', internal_monologue='Rolling a d20... The result is: 14.'), FunctionCallMessage(id='message-4c4839aa-981b-495d-bfb8-c707a3d7f7fc', date=datetime.datetime(2024, 12, 5, 6, 40, 38, 230151, tzinfo=datetime.timezone.utc), message_type='function_call', function_call=FunctionCall(name='roll_d20', arguments='{\\n  \"request_heartbeat\": true\\n}', function_call_id='267c02eb-da92-4e27-8ef7-574f2')), FunctionReturn(id='message-825df9aa-6e6c-41c7-be5a-08988441493d', date=datetime.datetime(2024, 12, 5, 6, 40, 38, 233546, tzinfo=datetime.timezone.utc), message_type='function_return', function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"You rolled a 1\",\\n  \"time\": \"2024-12-04 10:40:38 PM PST-0800\"\\n}', status='success', function_call_id='267c02eb-da92-4e27-8ef7-574f2')]\n"
     ]
    }
   ],
   "source": [
    "# Message an agent\n",
    "response = client.send_message(agent_id=agent_state.id, role=\"user\", message=\"Roll the dice\")\n",
    "print(\"Usage\", response.usage)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Agent messages\", response.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec0aa1-f1de-41cf-947f-49e93f1bb280",
   "metadata": {},
   "source": [
    "## 2.3 Create Agent With Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca61fc-b823-4754-9a80-0e0403cacbe0",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "32f24cef-f703-4acb-8e56-e7895c2e02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from letta import create_client, LLMConfig, EmbeddingConfig\n",
    "from letta.schemas.memory import ChatMemory\n",
    "client = create_client() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a950987-70fe-46d2-a115-70083973b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting AgentId: agent-31d90ecd-f140-4d2c-a6cb-b50f9e76c958\n",
      "Deleting AgentId: agent-9aa393ad-4924-41f2-8223-4aa7e8c296db\n"
     ]
    }
   ],
   "source": [
    "for agent_state in client.list_agents():\n",
    "    print(f'Deleting AgentId: {agent_state.id}')\n",
    "    client.delete_agent(agent_id=agent_state.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52668d04-76ba-4b18-837f-7e1697746550",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58b4e1b0-833e-42ae-b45d-2c142640cdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: label\n",
      "Kind: POSITIONAL_OR_KEYWORD\n",
      "Default: <class 'inspect._empty'>\n",
      "------------------------------\n",
      "Name: text\n",
      "Kind: POSITIONAL_OR_KEYWORD\n",
      "Default: <class 'inspect._empty'>\n",
      "------------------------------\n",
      "Name: template_name\n",
      "Kind: POSITIONAL_OR_KEYWORD\n",
      "Default: None\n",
      "------------------------------\n",
      "Name: template\n",
      "Kind: POSITIONAL_OR_KEYWORD\n",
      "Default: False\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "def inspect_function_signature(fn):\n",
    "    signature = inspect.signature(fn)\n",
    "    for param_name, param in signature.parameters.items():\n",
    "        print(f\"Name: {param_name}\")\n",
    "        print(f\"Kind: {param.kind}\")\n",
    "        print(f\"Default: {param.default}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "inspect_function_signature(client.create_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ae9168d-ca02-492e-aaf4-b484b4d95ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class_constructor(cls):\n",
    "    # Get the __init__ method of the class\n",
    "    init_method = getattr(cls, \"__init__\", None)\n",
    "    if not init_method:\n",
    "        print(f\"The class {cls.__name__} does not have a constructor (__init__).\")\n",
    "        return\n",
    "\n",
    "    # Get the function signature of the constructor\n",
    "    signature = inspect.signature(init_method)\n",
    "    print(f\"Constructor for class: {cls.__name__}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Loop through parameters and print details\n",
    "    for param_name, param in signature.parameters.items():\n",
    "        print(f\"Name: {param_name}\")\n",
    "        print(f\"Kind: {param.kind}\")\n",
    "        print(f\"Default: {param.default if param.default != inspect.Parameter.empty else 'No Default'}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19067d7-9540-4967-9f42-2bb8375c59a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe7a43bc-685c-4f7f-9364-d64f0e63b6a1",
   "metadata": {},
   "source": [
    "### Type: BasicBlockMemory\n",
    "- Represents a set of Memory Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9f1620ef-5452-4614-86fc-ff30bc5df8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from letta.schemas.memory import BasicBlockMemory\n",
    "\n",
    "client = create_client() \n",
    "\n",
    "org_block = client.create_block(text=\"Organization: Letta\", label=\"org\") \n",
    "persona_block = client.create_block(text=\"I am docbot and must answer user questions.\", label=\"persona\")  \n",
    "\n",
    "llm_ollama = LLMConfig(\n",
    "    # llama model versions: https://github.com/ollama/ollama\n",
    "    model=\"llama3.2:1b\", #\"llama3.2\", #ollama run llama3.2:1b\n",
    "    model_endpoint_type=\"ollama\", # llamacp\n",
    "    model_endpoint=ollama_url,\n",
    "    context_window=16384\n",
    ")\n",
    "\n",
    "agent_state_blockmem = client.create_agent(\n",
    "    memory=BasicBlockMemory(\n",
    "        blocks = [org_block, persona_block]\n",
    "    ),\n",
    "    llm_config=llm_ollama,\n",
    "    embedding_config=EmbeddingConfig.default_config(model_name=\"text-embedding-ada-002\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7318c6b-9d4b-4657-91cb-9230f479a39d",
   "metadata": {},
   "source": [
    "#### Try to showcase memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "24341727-c129-4a58-bb9b-8fb3f1fa9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_message(response):\n",
    "    print('\\n---------------------------------------\\n')\n",
    "    print(\"\\tUsage\", response.usage)\n",
    "\n",
    "    print(\"\")\n",
    "    for m in response.messages:\n",
    "        print(f'\\t{m}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2d16f865-b715-4bc1-a613-dfcb9a94fd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(value='I am docbot and must answer user questions.', limit=2000, template_name=None, template=False, label='persona', description=None, metadata_={}, user_id='user-00000000-0000-4000-8000-000000000000', id='block-164f3fb9-0119-4735-b6bf-2c3e86a7a2fc')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_memory.get_block(\"persona\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e31d6f8d-7d97-404e-90be-ab283fb1c7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------\n",
      "\n",
      "\tUsage completion_tokens=85 prompt_tokens=2618 total_tokens=2688 step_count=1\n",
      "\n",
      "\tid='message-b0fc3f03-bf06-4f6c-8aa4-d7d7f6629ded' date=datetime.datetime(2024, 12, 6, 19, 10, 30, 87423, tzinfo=datetime.timezone.utc) message_type='internal_monologue' internal_monologue=\"I've seen the contrast between old and new companies. It highlights the importance of adaptability in business.\"\n",
      "\n",
      "\tid='message-b0fc3f03-bf06-4f6c-8aa4-d7d7f6629ded' date=datetime.datetime(2024, 12, 6, 19, 10, 30, 87423, tzinfo=datetime.timezone.utc) message_type='function_call' function_call=FunctionCall(name='send_message', arguments='{\\n  \"message\": \"That\\'s a great point! Adaptability is crucial for survival in today\\'s fast-paced market.\"\\n}', function_call_id='b4f567c1-7b9e-4963-a5e1-dd35b')\n",
      "\n",
      "\tid='message-25aaabf4-8480-4665-a392-c7fa169baa04' date=datetime.datetime(2024, 12, 6, 19, 10, 30, 89650, tzinfo=datetime.timezone.utc) message_type='function_return' function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-12-06 11:10:30 AM PST-0800\"\\n}' status='success' function_call_id='b4f567c1-7b9e-4963-a5e1-dd35b'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_message(client.send_message(agent_id=agent_state_blockmem.id, role=\"user\", message=\"I currently work at Salesforce.  Previously, it was Ebay.  Compare the 2 companies.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "09960a6a-cc43-4e75-841a-fe2380d0cd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage completion_tokens=77 prompt_tokens=2416 total_tokens=2478 step_count=1\n",
      "\n",
      "\tid='message-9a75beb8-6892-4f1b-aa86-000b552a3806' date=datetime.datetime(2024, 12, 6, 19, 8, 51, 682358, tzinfo=datetime.timezone.utc) message_type='internal_monologue' internal_monologue=\"I've worked at both Ebay and Salesforce. It's fascinating to see how companies evolve over time.\"\n",
      "\n",
      "\tid='message-9a75beb8-6892-4f1b-aa86-000b552a3806' date=datetime.datetime(2024, 12, 6, 19, 8, 51, 682358, tzinfo=datetime.timezone.utc) message_type='function_call' function_call=FunctionCall(name='send_message', arguments='{\\n  \"message\": \"A great observation! It shows the dynamic nature of business.\"\\n}', function_call_id='21a47749-6cbf-4c82-8dc6-c9588')\n",
      "\n",
      "\tid='message-87b5d0c6-7f89-4339-8ef6-1bb1f9b2b9c0' date=datetime.datetime(2024, 12, 6, 19, 8, 51, 686215, tzinfo=datetime.timezone.utc) message_type='function_return' function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-12-06 11:08:51 AM PST-0800\"\\n}' status='success' function_call_id='21a47749-6cbf-4c82-8dc6-c9588'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_message(client.send_message(agent_id=agent_state_blockmem.id, role=\"user\", message=\"Which company did I work before Salesforce?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f677a62c-99ea-425a-b111-5f640916a19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .message-container, .usage-container {\n",
       "                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "                max-width: 800px;\n",
       "                margin: 20px auto;\n",
       "                background-color: #1e1e1e;\n",
       "                border-radius: 8px;\n",
       "                overflow: hidden;\n",
       "                color: #d4d4d4;\n",
       "            }\n",
       "            .message, .usage-stats {\n",
       "                padding: 10px 15px;\n",
       "                border-bottom: 1px solid #3a3a3a;\n",
       "            }\n",
       "            .message:last-child, .usage-stats:last-child {\n",
       "                border-bottom: none;\n",
       "            }\n",
       "            .title {\n",
       "                font-weight: bold;\n",
       "                margin-bottom: 5px;\n",
       "                color: #ffffff;\n",
       "                text-transform: uppercase;\n",
       "                font-size: 0.9em;\n",
       "            }\n",
       "            .content {\n",
       "                background-color: #2d2d2d;\n",
       "                border-radius: 4px;\n",
       "                padding: 5px 10px;\n",
       "                font-family: 'Consolas', 'Courier New', monospace;\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "            .json-key, .function-name, .json-boolean { color: #9cdcfe; }\n",
       "            .json-string { color: #ce9178; }\n",
       "            .json-number { color: #b5cea8; }\n",
       "            .internal-monologue { font-style: italic; }\n",
       "        </style>\n",
       "        <div class=\"message-container\">\n",
       "        \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "                <div class=\"content\"><span class=\"internal-monologue\">I&#x27;ve worked at both Ebay and Salesforce. It&#x27;s fascinating to see how companies evolve over time.</span></div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION CALL</div>\n",
       "                <div class=\"content\"><span class=\"function-name\">send_message</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"message\"</span>: <span class=\"json-string\">\"A great observation! It shows the dynamic nature of business.\"</span><br>})</div>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"message\">\n",
       "                <div class=\"title\">FUNCTION RETURN</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"status\"</span>: <span class=\"json-key\">\"OK\",<br>&nbsp;&nbsp;\"message\"</span>: <span class=\"json-key\">\"None\",<br>&nbsp;&nbsp;\"time\"</span>: <span class=\"json-string\">\"2024-12-06 01:00:57 PM PST-0800\"</span><br>}</div>\n",
       "            </div>\n",
       "            </div>\n",
       "        <div class=\"usage-container\">\n",
       "            <div class=\"usage-stats\">\n",
       "                <div class=\"title\">USAGE STATISTICS</div>\n",
       "                <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"completion_tokens\"</span>: <span class=\"json-number\">77</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"prompt_tokens\"</span>: <span class=\"json-number\">2828</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"total_tokens\"</span>: <span class=\"json-number\">2987</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"step_count\"</span>: <span class=\"json-number\">1</span><br>}</div>\n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "LettaResponse(messages=[InternalMonologue(id='message-efcd647d-f8ba-489a-91da-36ab58ca41b2', date=datetime.datetime(2024, 12, 6, 21, 0, 57, 366947, tzinfo=datetime.timezone.utc), message_type='internal_monologue', internal_monologue=\"I've worked at both Ebay and Salesforce. It's fascinating to see how companies evolve over time.\"), FunctionCallMessage(id='message-efcd647d-f8ba-489a-91da-36ab58ca41b2', date=datetime.datetime(2024, 12, 6, 21, 0, 57, 366947, tzinfo=datetime.timezone.utc), message_type='function_call', function_call=FunctionCall(name='send_message', arguments='{\\n  \"message\": \"A great observation! It shows the dynamic nature of business.\"\\n}', function_call_id='b7e9db5e-e9a4-4560-b1f2-f3119')), FunctionReturn(id='message-a3488535-71df-4b9d-9cca-205e76823b64', date=datetime.datetime(2024, 12, 6, 21, 0, 57, 368777, tzinfo=datetime.timezone.utc), message_type='function_return', function_return='{\\n  \"status\": \"OK\",\\n  \"message\": \"None\",\\n  \"time\": \"2024-12-06 01:00:57 PM PST-0800\"\\n}', status='success', function_call_id='b7e9db5e-e9a4-4560-b1f2-f3119')], usage=LettaUsageStatistics(completion_tokens=77, prompt_tokens=2828, total_tokens=2987, step_count=1))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.send_message(agent_id=agent_state_blockmem.id, role=\"user\", message=\"I currently work at Salesforce.  Previously, it was Ebay.  Compare the 2 companies.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c6ee3-c7da-428d-ae23-1faa7c944f9d",
   "metadata": {},
   "source": [
    "### Type: Blocks\n",
    "- Lowest building level; Set of block makes up core memory\n",
    "- A standalone block can belong to multiple memory classes, which is a way for multiple agents to share memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d14e064d-c0fb-494a-878b-0ca472cbf717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(value='Name: Sarah', limit=2000, template_name=None, template=False, label='human', description=None, metadata_={}, user_id='user-00000000-0000-4000-8000-000000000000', id='block-6419face-47eb-4148-80f3-d31546920042')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a standalone block\n",
    "block = client.create_block(text= \"Name: Sarah\", label= \"human\")\n",
    "\n",
    "# This implicitly adds to the active agent (aka agent_state_blockmem) memory    \n",
    "\n",
    "# agent 1 memory\n",
    "memory1 = BasicBlockMemory(blocks=[block])\n",
    "\n",
    "# agent 2 memory\n",
    "memory2 = BasicBlockMemory(blocks=[block])\n",
    "\n",
    "# I will call this in next section after I create an agent that use this memory\n",
    "core_memory = client.get_core_memory(agent_state_blockmem.id)\n",
    "# retrieve the block with label \"human\" \n",
    "core_memory.get_block(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11dc22-205e-4d72-9d6d-453eb9c3a6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5501f763-370c-47b1-9c10-5b853b7e2156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75025e-6f6a-4fb2-ba69-bf2886d27fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fa08139-d0a8-49b6-b5de-77f1a8e93d56",
   "metadata": {},
   "source": [
    "### Type: Chat Memory\n",
    "- By default, agents have a ChatMemory memory class, which is designed for a 1:1 chat between a human and agent. You can use the persona section of ChatMemory to customize the prompt for your agent, and the human section to add personalization data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9bc5bde1-f581-430b-814d-cfb35f511c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from letta import create_client, LLMConfig, EmbeddingConfig\n",
    "from letta.schemas.memory import ChatMemory\n",
    "\n",
    "llm_ollama = LLMConfig(\n",
    "    # llama model versions: https://github.com/ollama/ollama\n",
    "    model=\"llama3.2:1b\", #\"llama3.2\", #ollama run llama3.2:1b\n",
    "    model_endpoint_type=\"ollama\", # llamacp\n",
    "    model_endpoint=ollama_url,\n",
    "    context_window=16384\n",
    ")\n",
    "\n",
    "client = client if client else create_client() \n",
    "\n",
    "agent_state_with_chatmemory = client.create_agent(\n",
    "    memory=ChatMemory(\n",
    "        persona=\"I am docbot and must answer user questions.\", \n",
    "        human=\"Name: Sarah\"\n",
    "    ), \n",
    "    llm_config=llm_ollama,\n",
    "    embedding_config=EmbeddingConfig.default_config(model_name=\"text-embedding-ada-002\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dbad909a-e1b4-487d-98a6-eeaee819465b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Agent agent_id=agent-e1215df4-7360-41af-abdd-3f17ebc425ac does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_state_with_chatmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat exercises should I do to strengthen my knees.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#print(\"Usage\", response.usage)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#print(\"\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#for m in response.messages:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#    print(f'{m}\\n')\u001b[39;00m\n\u001b[1;32m      8\u001b[0m response\n",
      "File \u001b[0;32m~/Documents/dev/git/project/walle/venv/lib/python3.11/site-packages/letta/client/client.py:2051\u001b[0m, in \u001b[0;36mLocalClient.send_message\u001b[0;34m(self, message, role, name, agent_id, agent_name, stream_steps, stream_tokens, include_full_message)\u001b[0m\n\u001b[1;32m   2048\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m-> 2051\u001b[0m usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMessageCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessageRole\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;66;03m# auto-save\u001b[39;00m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_save:\n",
      "File \u001b[0;32m~/Documents/dev/git/project/walle/venv/lib/python3.11/site-packages/letta/server/server.py:714\u001b[0m, in \u001b[0;36mSyncServer.send_messages\u001b[0;34m(self, user_id, agent_id, messages, wrap_user_message, wrap_system_message)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser user_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mms\u001b[38;5;241m.\u001b[39mget_agent(agent_id\u001b[38;5;241m=\u001b[39magent_id, user_id\u001b[38;5;241m=\u001b[39muser_id) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent agent_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    716\u001b[0m message_objects: List[Message] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(m, MessageCreate) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages):\n",
      "\u001b[0;31mValueError\u001b[0m: Agent agent_id=agent-e1215df4-7360-41af-abdd-3f17ebc425ac does not exist"
     ]
    }
   ],
   "source": [
    "response = client.send_message(agent_id=agent_state_with_chatmemory.id, role=\"user\", message=\"What exercises should I do to strengthen my knees.\")\n",
    "\n",
    "#print(\"Usage\", response.usage)\n",
    "\n",
    "#print(\"\")\n",
    "#for m in response.messages:\n",
    "#    print(f'{m}\\n')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb3e8e-d79a-40e8-adfb-aeed34c12b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
